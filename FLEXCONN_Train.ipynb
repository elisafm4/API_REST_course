{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FLEXCONN_Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elisafm4/API_REST_course/blob/master/FLEXCONN_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ayGLS4BFixO",
        "colab_type": "text"
      },
      "source": [
        "# 1. Procesado Imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhg3yCm3Fpff",
        "colab_type": "text"
      },
      "source": [
        "Cargamos imágenes y hacemos preprocesado (a nivel local).\n",
        "\n",
        "Obtenemos el directorio de las imágenes y el número de pacientes (N)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rV5a3hLFjwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "dir_name = \"LS_FLEXCONN\"\n",
        "atlas = os.listdir(dir_name)\n",
        "\n",
        "nums = []\n",
        "\n",
        "for im in atlas:\n",
        "    nums.append(re.findall('\\d+', im)[0])\n",
        "\n",
        "N = np.unique(nums)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Pf4SdRFvhN",
        "colab_type": "text"
      },
      "source": [
        "Para cada paciente, aplicamos un skull-stripping para eliminar lo que no pertenece al cerebro tanto en T1 como en FLAIR y reducir posibles falsos positivos. Los nuevos volúmenes junto con el volumen de las lesiones son guardados en un nuevo directorio 'Atlas'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2tQ4aDqFj7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nibabel as nib\n",
        "import shutil\n",
        "\n",
        "for i in N:\n",
        "    volT1 = nib.load(\"{}/atlas{}_T1.nii.gz\".format(dir_name, i))\n",
        "    volFL = nib.load(\"{}/atlas{}_FL.nii.gz\".format(dir_name, i))\n",
        "    mask = nib.load(\"{}/atlas{}_T1_mask.nii.gz\".format(dir_name, i)).get_fdata()\n",
        "    T1_pre = volT1.get_fdata() * mask\n",
        "    FL_pre = volFL.get_fdata() * mask\n",
        "    T1_pre_nii = nib.Nifti1Image(T1_pre, volT1.affine)\n",
        "    FL_pre_nii = nib.Nifti1Image(FL_pre, volFL.affine)\n",
        "    T1_pre_nii.to_filename(\"Atlas/atlas{}_T1.nii.gz\".format(i))\n",
        "    FL_pre_nii.to_filename(\"Atlas/atlas{}_FL.nii.gz\".format(i))\n",
        "    shutil.copy(\"{}/atlas{}_mask.nii.gz\".format(dir_name, i), \"Atlas/atlas{}_mask.nii.gz\".format(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH3puEz5Fx-n",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, selecionamos al azar un 20% de los pacientes para test y un 80% para train, que son guardados en carpetas separadas 'Atlas_train' y 'Atlas_test' respectivamente. Estas carpetas son las que se suben al directorio del drive ***IMAGENES***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm7CkONgFkCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "n = int(0.2 * len(N))\n",
        "random.seed(1000)\n",
        "\n",
        "test = random.sample(N.tolist(), n)\n",
        "train = list(set(N) - set(test))\n",
        "\n",
        "for i in test:\n",
        "    shutil.copy(\"Atlas/atlas{}_mask.nii.gz\".format(i), \"Atlas_test/atlas{}_mask.nii.gz\".format(i))\n",
        "    shutil.copy(\"Atlas/atlas{}_T1.nii.gz\".format(i), \"Atlas_test/atlas{}_T1.nii.gz\".format(i))\n",
        "    shutil.copy(\"Atlas/atlas{}_FL.nii.gz\".format(i), \"Atlas_test/atlas{}_FL.nii.gz\".format(i))\n",
        "    \n",
        "for i in train:\n",
        "    shutil.copy(\"Atlas/atlas{}_mask.nii.gz\".format(i), \"Atlas_train/atlas{}_mask.nii.gz\".format(i))\n",
        "    shutil.copy(\"Atlas/atlas{}_T1.nii.gz\".format(i), \"Atlas_train/atlas{}_T1.nii.gz\".format(i))\n",
        "    shutil.copy(\"Atlas/atlas{}_FL.nii.gz\".format(i), \"Atlas_train/atlas{}_FL.nii.gz\".format(i))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf3ZFdurF5CW",
        "colab_type": "text"
      },
      "source": [
        "# 2. Código FLEXCONN sin manipular"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBrLvRtmF5e_",
        "colab_type": "text"
      },
      "source": [
        "Utilizamos el código de FLEXCONN. Como el entrenamiento requiere bastantes horas, se realizarán checkpoins del código que serán guardados en drive ***TMF/FLEXCONN_CODE/Checkpoints***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDMyVR67kW1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allow_growth = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcYIezG3RyDt",
        "colab_type": "code",
        "outputId": "d8b45e22-6e15-4855-cc26-15864a1f7509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip uninstall  -y tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0rc2:\n",
            "  Successfully uninstalled tensorflow-2.2.0rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQOFXkwdSh10",
        "colab_type": "text"
      },
      "source": [
        "Cambiamos el enterno a GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY7fYYFfTHI7",
        "colab_type": "code",
        "outputId": "454e51a8-2198-4a1b-dc43-0a7fed878585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip uninstall tensorflow-gpu"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ri9VDn4Ryoa",
        "colab_type": "code",
        "outputId": "3f7aa5c6-9214-4f66-eed7-c5afbe6dc2af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K     |████████████████████████████████| 332.1MB 48kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.27.2)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 36.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.34.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 41.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.18.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.3.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.8)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (46.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.10.0)\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, tensorflow-gpu\n",
            "Successfully installed tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ae8sWrlSYn8",
        "colab_type": "code",
        "outputId": "2b7a8158-4902-447b-f14e-1df1371a1aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL6bPlACWubf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  import tensorflow.compat.v2 as tf\n",
        "except Exception:\n",
        "  pass\n",
        "tf.enable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnOaZ-r1T2k4",
        "colab_type": "code",
        "outputId": "d23e6fd6-48ae-4b3b-e696-1d40c9bf2595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.test.is_gpu_available()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_CQPNUL3nLN",
        "colab_type": "text"
      },
      "source": [
        "## TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7Mf1q8X2myn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, re, time, json\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7MQcB1C0lbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "IS_COLAB_BACKEND = 'COLAB_GPU' in os.environ  # this is always set on Colab, the value is 0 or 1 depending on GPU presence\n",
        "if IS_COLAB_BACKEND:\n",
        "  from google.colab import auth\n",
        "  # Authenticates the Colab machine and also the TPU using your\n",
        "  # credentials so that they can access your private GCS buckets.\n",
        "  auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isjJAdcc3pV9",
        "colab_type": "code",
        "outputId": "39db7e7f-72e0-4491-80d4-e8a13ce0639f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "tpu_strategy.num_replicas_in_sync"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  ['10.42.217.194:8470']\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.42.217.194:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.42.217.194:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zog5Pnz72wNe",
        "colab_type": "text"
      },
      "source": [
        "## TPU or GPU detection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmj5IJsa2v2f",
        "colab_type": "code",
        "outputId": "ce79fe49-de16-4420-ccf7-e851932f9e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "except ValueError:\n",
        "  tpu = None\n",
        "  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "    \n",
        "# Select appropriate distribution strategy\n",
        "if tpu:\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu, steps_per_run=128) # Going back and forth between TPU and host is expensive. Better to run 128 batches on the TPU before reporting back.\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \n",
        "elif len(gpus) > 1:\n",
        "  strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
        "  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
        "elif len(gpus) == 1:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on single GPU ', gpus[0].name)\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on CPU')\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver at 0x7f8f8d37a048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuobojkOUKzl",
        "colab_type": "text"
      },
      "source": [
        "Conectamos *google-colab* con los archivos del Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9tjLRXJFkFX",
        "colab_type": "code",
        "outputId": "e6ead74d-b721-4c88-9542-60c602462429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaR75op2J0x1",
        "colab_type": "code",
        "outputId": "b0221ca1-caa7-4473-9500-3c44b7df3692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!pip3 install -r \"/content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt (line 1)) (1.18.2)\n",
            "Requirement already satisfied: nibabel>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: statsmodels>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt (line 3)) (0.10.2)\n",
            "Requirement already satisfied: tqdm>=4.15.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt (line 4)) (4.38.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: scikit_learn>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.8.0->-r /content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt (line 3)) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.8.0->-r /content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt (line 3)) (1.0.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit_learn>=0.19.0->-r /content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt (line 6)) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.0->statsmodels>=0.8.0->-r /content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt (line 3)) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels>=0.8.0->-r /content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels>=0.8.0->-r /content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/requirements.txt (line 3)) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_n29jEnhg0V",
        "colab_type": "code",
        "outputId": "17eae413-c16e-47bd-932f-d37f48dc8454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import statsmodels.api as sm\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras import backend,losses\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.python.keras import Input, Model\n",
        "from tensorflow.python.keras.layers import (AveragePooling2D, AveragePooling3D, Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, concatenate, Lambda)\n",
        "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
        "from tensorflow.python.keras.models import load_model\n",
        "import sys\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from scipy import ndimage\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.signal import argrelextrema\n",
        "\n",
        "backend.set_floatx('float32')\n",
        "backend.set_image_data_format('channels_last')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M126xtx0i_Na",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def multi_gpu_model(model, gpus):\n",
        "\n",
        "    if isinstance(gpus, (list, tuple)):\n",
        "        num_gpus = len(gpus)\n",
        "        target_gpu_ids = gpus\n",
        "    else:\n",
        "        num_gpus = gpus\n",
        "        target_gpu_ids = range(num_gpus)\n",
        "    \n",
        "    def get_slice(data, num, parts):\n",
        "        shape = tf.shape(data)\n",
        "        batch_size = shape[:1]\n",
        "        in_shape = shape[1:]\n",
        "        step = batch_size // parts\n",
        "        if num == num_gpus - 1:\n",
        "            size = batch_size - step * num\n",
        "        else:\n",
        "            size = step\n",
        "        size = tf.concat([size, in_shape], axis=0)\n",
        "        stride = tf.concat([step, in_shape * 0], axis=0)\n",
        "        start = stride * num\n",
        "        return tf.slice(data, start, size)\n",
        "    \n",
        "    all_outputs = []\n",
        "    for i in range(len(model.outputs)):\n",
        "        all_outputs.append([])\n",
        "    \n",
        "    # Place a copy of the model on each GPU,\n",
        "    # each getting a slice of the inputs.\n",
        "    for i, gpu_id in enumerate(target_gpu_ids):\n",
        "        with tf.device('/gpu:%d' % gpu_id):\n",
        "            with tf.name_scope('replica_%d' % gpu_id):\n",
        "                inputs = []\n",
        "                # Retrieve a slice of the input.\n",
        "                for x in model.inputs:\n",
        "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
        "                    slice_i = Lambda(get_slice,\n",
        "                                     output_shape=input_shape,\n",
        "                                     arguments={'num': i,\n",
        "                                                'parts': num_gpus})(x)\n",
        "                    inputs.append(slice_i)\n",
        "                \n",
        "                # Apply model on slice\n",
        "                # (creating a model replica on the target device).\n",
        "                outputs = model(inputs)\n",
        "                if not isinstance(outputs, list):\n",
        "                    outputs = [outputs]\n",
        "                \n",
        "                # Save the outputs for merging back together later.\n",
        "                for o in range(len(outputs)):\n",
        "                    all_outputs[o].append(outputs[o])\n",
        "    \n",
        "    # Merge outputs on CPU.\n",
        "    with tf.device('/cpu:0'):\n",
        "        merged = []\n",
        "        for name, outputs in zip(model.output_names, all_outputs):\n",
        "            merged.append(concatenate(outputs,\n",
        "                                      axis=0, name=name))\n",
        "    return Model(model.inputs, merged)\n",
        "\n",
        "\n",
        "def pad_image(vol, padsize):\n",
        "    dim = vol.shape\n",
        "    padsize = np.asarray(padsize, dtype=int)\n",
        "    dim2 = dim + 2 * padsize\n",
        "    temp = np.zeros(dim2, dtype=np.float32)\n",
        "    temp[padsize:dim[0] + padsize, padsize:dim[1] + padsize, padsize:dim[2] + padsize] = vol\n",
        "    return temp\n",
        "\n",
        "\n",
        "def normalize_image(vol, contrast):\n",
        "    # All MR images must be non-negative. Sometimes cubic interpolation may introduce negative numbers.\n",
        "    # This will also affect if the image is CT, which not considered here. Non-negativity is required\n",
        "    # while getting patches, where nonzero voxels are considered to collect patches.\n",
        "    vol[vol<0] = 0\n",
        "    temp = vol[np.nonzero(vol)].astype(float)\n",
        "    q = np.percentile(temp, 99)\n",
        "    temp = temp[temp <= q]\n",
        "    temp = temp.reshape(-1, 1)\n",
        "    bw = q / 80\n",
        "    print(\"99th quantile is %.4f, gridsize = %.4f\" % (q, bw))\n",
        "\n",
        "    kde = sm.nonparametric.KDEUnivariate(temp)\n",
        "\n",
        "    kde.fit(kernel='gau', bw=bw, gridsize=80, fft=True)\n",
        "    x_mat = 100.0 * kde.density\n",
        "    y_mat = kde.support\n",
        "\n",
        "    indx = argrelextrema(x_mat, np.greater)\n",
        "    indx = np.asarray(indx, dtype=int)\n",
        "    heights = x_mat[indx][0]\n",
        "    peaks = y_mat[indx][0]\n",
        "    peak = 1.00\n",
        "    print(\"%d peaks found.\" % (len(peaks)))\n",
        "\n",
        "\n",
        "    if contrast.lower() == \"t1\" or contrast.lower() == \"t1c\":\n",
        "        print(\"Double checking peaks with a GMM.\")\n",
        "        gmm = GaussianMixture(n_components=3, covariance_type='spherical', tol=0.001,\n",
        "                reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', precisions_init=None,\n",
        "                weights_init=(0.33, 0.33, 0.34), means_init=np.reshape((0.2 * q, 0.5 * q, 0.95 * q), (3, 1)),\n",
        "                warm_start=False, verbose=1, verbose_interval=1)\n",
        "        gmm.fit(temp.reshape(-1, 1))\n",
        "        m = gmm.means_[2]\n",
        "        peak = peaks[-1]\n",
        "        if m / peak < 0.75 or m / peak > 1.25:\n",
        "            print(\"WARNING: WM peak could be incorrect (%.4f vs %.4f). Please check.\" % (m, peak))\n",
        "            peaks = m\n",
        "        peak = peaks[-1]\n",
        "        print(\"Peak found at %.4f for %s\" % (peak, contrast))\n",
        "    elif contrast.lower() in ['t2', 'pd', 'fl', 'flc']:\n",
        "        peak_height = np.amax(heights)\n",
        "        idx = np.where(heights == peak_height)\n",
        "        peak = peaks[idx]\n",
        "        print(\"Peak found at %.4f for %s\" % (peak, contrast))\n",
        "    else:\n",
        "        print(\"Contrast must be either T1,T1C,T2,PD,FL, or FLC. You entered %s. Returning 1.\" % contrast)\n",
        "    return vol/peak\n",
        "\n",
        "\n",
        "def get_patches(vol4d, mask, opt):\n",
        "\n",
        "    patchsize = opt['patchsize']\n",
        "    nummodal = len(opt['modalities'])\n",
        "    maxpatch = opt['max_patches']\n",
        "    patchsize = np.asarray(patchsize, dtype=int)\n",
        "    dsize = np.floor(patchsize / 2).astype(dtype=int)\n",
        "    mask = np.asarray(mask, dtype=np.float32)\n",
        "    rng = random.SystemRandom()\n",
        "\n",
        "    if opt['loss'] == 'mse' or opt['loss'] == 'mae':\n",
        "        if len(patchsize) == 3:\n",
        "            blurmask = ndimage.filters.gaussian_filter(mask, sigma=(1, 1, 1))\n",
        "        else:\n",
        "            blurmask = np.zeros(mask.shape, dtype=np.float32)\n",
        "            for t in range(0, mask.shape[2]):\n",
        "                if np.ndarray.sum(mask[:, :, t]) > 0:\n",
        "                    blurmask[:, :, t] = ndimage.filters.gaussian_filter(mask[:, :, t], sigma=(1, 1))\n",
        "\n",
        "        blurmask = np.ndarray.astype(blurmask, dtype=np.float32)\n",
        "        blurmask[blurmask < 0.0001] = 0\n",
        "        blurmask = blurmask * 100  # Just to have reasonable looking error values during training, no other reason.\n",
        "    else:\n",
        "        blurmask = mask\n",
        "    \n",
        "    indx = np.nonzero(mask) # indx for positive patches\n",
        "    indx = np.asarray(indx, dtype=int)\n",
        "\n",
        "\n",
        "    num_patches = np.minimum(maxpatch, len(indx[0]))\n",
        "    print('Number of patches used  = %d (out of %d, maximum %d)' % (num_patches, len(indx[0]), maxpatch))\n",
        "    randindx = random.sample(range(0, len(indx[0])), num_patches)\n",
        "    newindx = np.ndarray((3, num_patches))\n",
        "    for i in range(0, num_patches):\n",
        "        for j in range(0, 3):\n",
        "            newindx[j, i] = indx[j, randindx[i]]\n",
        "    newindx = np.asarray(newindx, dtype=int)\n",
        "\n",
        "    # Add some negative samples as well\n",
        "    r = 1 # Sampling ratio\n",
        "    temp = copy.deepcopy(vol4d[:, :, :, 0])\n",
        "    temp[temp > 0] = 1\n",
        "    temp[temp <= 0] = 0\n",
        "    temp = np.multiply(temp, 1 - mask)\n",
        "    indx0 = np.nonzero(temp)\n",
        "    indx0 = np.asarray(indx0, dtype=int)\n",
        "    L = len(indx0[0])\n",
        "\n",
        "    # Sample equal number of negative patches\n",
        "    randindx0 = rng.sample(range(0, L), r * num_patches)\n",
        "    newindx0 = np.ndarray((3, r * num_patches))\n",
        "    for i in range(0, r * num_patches):\n",
        "        for j in range(0, 3):\n",
        "            newindx0[j, i] = indx0[j, randindx0[i]]\n",
        "    newindx0 = np.asarray(newindx0, dtype=int)\n",
        "\n",
        "    newindx = np.concatenate([newindx, newindx0], axis=1)\n",
        "\n",
        "\n",
        "    if len(patchsize) == 2:\n",
        "        matsize1 = ((r+1) * num_patches, patchsize[0], patchsize[1], 1)\n",
        "        matsize2 = ((r+1) * num_patches, patchsize[0], patchsize[1], nummodal)\n",
        "        \n",
        "        image_patches = np.ndarray(matsize2, dtype=np.float32)\n",
        "        mask_patches = np.ndarray(matsize1, dtype=np.float32)\n",
        "        \n",
        "        for i in range(0, (r+1)*num_patches):\n",
        "            idx1 = newindx[0, i]\n",
        "            idx2 = newindx[1, i]\n",
        "            idx3 = newindx[2, i]\n",
        "            \n",
        "            for m in range(0, nummodal):\n",
        "                image_patches[i, :, :, m] = vol4d[idx1 - dsize[0]:idx1 + dsize[0] + 1,\n",
        "                                                  idx2 - dsize[1]:idx2 + dsize[1] + 1, idx3, m]\n",
        "            mask_patches[i, :, :, 0] = blurmask[idx1 - dsize[0]:idx1 + dsize[0] + 1,\n",
        "                                                idx2 - dsize[1]:idx2 + dsize[1] + 1, idx3]\n",
        "    else:\n",
        "        matsize1 = ((r+1)*num_patches, patchsize[0], patchsize[1], patchsize[2], 1)\n",
        "        matsize2 = ((r+1)*num_patches, patchsize[0], patchsize[1], patchsize[2], nummodal)\n",
        "        \n",
        "        image_patches = np.ndarray(matsize2, dtype=np.float32)\n",
        "        mask_patches = np.ndarray(matsize1, dtype=np.float32)\n",
        "        \n",
        "        for i in range(0, (r+1)*num_patches):\n",
        "            idx1 = newindx[0, i]\n",
        "            idx2 = newindx[1, i]\n",
        "            idx3 = newindx[2, i]\n",
        "            \n",
        "            for m in range(0, nummodal):\n",
        "                image_patches[i, :, :, :, m] = vol4d[idx1 - dsize[0]:idx1 + dsize[0] + 1,\n",
        "                                                     idx2 - dsize[1]:idx2 + dsize[1] + 1,\n",
        "                                                     idx3 - dsize[2]:idx3 + dsize[2] + 1, m]\n",
        "            mask_patches[i, :, :, :, 0] = blurmask[idx1 - dsize[0]:idx1 + dsize[0] + 1,\n",
        "                                                   idx2 - dsize[1]:idx2 + dsize[1] + 1,\n",
        "                                                   idx3 - dsize[2]:idx3 + dsize[2] + 1]\n",
        "    return image_patches, mask_patches\n",
        "\n",
        "def check_nifti_filepath(directory, file_prefix):\n",
        "    filepath = os.path.join(directory, file_prefix + '.nii.gz')\n",
        "    filepath = filepath if os.path.exists(filepath) else os.path.join(directory, file_prefix + '.nii')\n",
        "    if not os.path.exists(filepath):\n",
        "        raise ValueError('File %s does not exists' % filepath)\n",
        "    return filepath\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yt6Uqu6i_QC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_inception_3d(inlayer, base_filters):\n",
        "    conv_inception1a = Conv3D(base_filters * 4, (1, 1, 1), activation='relu', padding='same')(inlayer)\n",
        "    \n",
        "    conv_inception2a = Conv3D(base_filters * 6, (1, 1, 1), activation='relu', padding='same')(inlayer)\n",
        "    conv_inception4a = Conv3D(base_filters * 8, (3, 3, 3), activation='relu', padding='same')(conv_inception2a)\n",
        "    \n",
        "    conv_inception3a = Conv3D(base_filters, (1, 1, 1), activation='relu', padding='same')(inlayer)\n",
        "    conv_inception5a = Conv3D(base_filters * 2, (5, 5, 5), activation='relu', padding='same')(conv_inception3a)\n",
        "    \n",
        "    pool_inception1a = AveragePooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), padding='same')(inlayer)\n",
        "    conv_inception6a = Conv3D(base_filters * 2, (1, 1, 1), activation='relu', padding='same')(pool_inception1a)\n",
        "    \n",
        "    pool_inception2a = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), padding='same')(inlayer)\n",
        "    conv_inception7a = Conv3D(base_filters * 2, (1, 1, 1), activation='relu', padding='same')(pool_inception2a)\n",
        "    print('concat')\n",
        "    outlayer = concatenate([conv_inception1a, conv_inception4a], axis=-1)\n",
        "    outlayer = concatenate([outlayer, conv_inception5a], axis=-1)\n",
        "    outlayer = concatenate([outlayer, conv_inception6a], axis=-1)\n",
        "    outlayer = concatenate([outlayer, conv_inception7a], axis=-1)\n",
        "    return outlayer\n",
        "\n",
        "\n",
        "def get_model_3d(base_filters, numchannel, gpus,loss):\n",
        "    inputs = Input((None, None, None, int(numchannel)))\n",
        "    conv1 = Conv3D(base_filters * 8, (3, 3, 3), activation='relu', padding='same', strides=(1, 1, 1))(inputs)\n",
        "    conv2 = Conv3D(base_filters * 8, (3, 3, 3), activation='relu', padding='same', strides=(1, 1, 1))(conv1)\n",
        "    print('first inception')\n",
        "    \n",
        "    inception1 = get_inception_3d(conv2, base_filters)\n",
        "    inception2 = get_inception_3d(inception1, base_filters)\n",
        "    inception3 = get_inception_3d(inception2, base_filters)\n",
        "    \n",
        "    convconcat1 = Conv3D(base_filters * 4, (3, 3, 3), activation='relu', padding='same', strides=(1, 1, 1))(inception3)\n",
        "    convconcat2 = Conv3D(base_filters * 4, (3, 3, 3), activation='relu', padding='same', strides=(1, 1, 1))(convconcat1)\n",
        "    print('check loss')\n",
        "    if loss == 'bce':\n",
        "        conv_last = Conv3D(1, (3, 3, 3), activation='sigmoid', padding='same', strides=(1, 1, 1))(convconcat2)\n",
        "    else:\n",
        "        conv_last = Conv3D(1, (3, 3, 3), activation='relu', padding='same', strides=(1, 1, 1))(convconcat2)\n",
        "    model = Model(inputs=inputs, outputs=conv_last)\n",
        "    print(gpus)\n",
        "    if (gpus) > 1:\n",
        "        #import tensorflow as tf\n",
        "        with tf.device('/cpu:0'):\n",
        "            model = multi_gpu_model(model, gpus)\n",
        "    print('compile model')\n",
        "    if loss == 'bce':\n",
        "        model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    elif loss == 'mae':\n",
        "        model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_absolute_error')\n",
        "    else:\n",
        "        model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tHQ9CKIxU8el",
        "colab": {}
      },
      "source": [
        "\n",
        "def main(kwargs):\n",
        "    \n",
        "    nummodal = int(len(kwargs['modalities']))\n",
        "    padsize = np.max(np.array(kwargs['patchsize']) + 1) / 2\n",
        "    kwargs['batchsize'] = kwargs['batchsize'] * kwargs['numgpu']\n",
        "    f = 0 # num_patches\n",
        "    #--efm\n",
        "    atlas = os.listdir(kwargs['atlasdir'])\n",
        "    nums = []\n",
        "    for im in atlas:\n",
        "        nums.append(re.findall('\\d+', im)[0])\n",
        "    N = np.unique(nums)\n",
        "    #---\n",
        "\n",
        "    for i in N:\n",
        "        maskname = check_nifti_filepath(kwargs['atlasdir'], ('atlas%s' % (i)) + '_' + 'mask')\n",
        "        f += min(kwargs['max_patches'], nib.load(maskname).get_fdata().sum())\n",
        "    \n",
        "    print('Total number of lesion patches = ' + str(int(f)))\n",
        "    r = 1 # ratio between positive and negative patches\n",
        "    mask_patches = np.zeros((int((r+1)*f),) + kwargs['patchsize'] + (1,), dtype=np.float32)\n",
        "    image_patches = np.zeros((int((r+1)*f),) + kwargs['patchsize'] + (nummodal,), dtype=np.float32)\n",
        "    \n",
        "    time_id = time.strftime('%d-%m-%Y_%H-%M-%S')\n",
        "    print('Unique ID is %s ' % time_id)\n",
        "\n",
        "    con = '+'.join([str(mod).upper() for mod in kwargs['modalities']])\n",
        "    psize = 'x'.join([str(side) for side in kwargs['patchsize']])\n",
        "    outname = 'FLEXCONN_Model_' + psize + '_Orient%d%d%d_' + con + '_' + time_id + '.h5'\n",
        "    \n",
        "    codes = [(0, 1, 2), (1, 2, 0), (2, 0, 1)]\n",
        "    \n",
        "    for orient in range(1 if kwargs['axial_only'] else 3):\n",
        "        transpose_code = codes[orient]\n",
        "        orient_outname = os.path.join(kwargs['outdir'], outname % transpose_code)\n",
        "        if kwargs['loss'] == 'bce':\n",
        "            tempoutname = orient_outname.replace('.h5', '_epoch-{epoch:03d}_acc-{val_accuracy:.4f}.h5')\n",
        "        else:\n",
        "            tempoutname = orient_outname.replace('.h5', '_epoch-{epoch:03d}_val_loss-{val_loss:.4f}.h5')\n",
        "\n",
        "        print('Model for orientation %s will be written at %s' % (str(transpose_code), orient_outname))\n",
        "\n",
        "        # Re-initialize total matrix size because some of them could be discarded during multi-gpu process\n",
        "        mask_patches = np.zeros((int((r+1) * f),) + kwargs['patchsize'] + (1,), dtype=np.float32)\n",
        "        image_patches = np.zeros((int((r+1) * f),) + kwargs['patchsize'] + (nummodal,), dtype=np.float32)\n",
        "\n",
        "        patch_count = 0\n",
        "        #---efm\n",
        "        for i in N: #---\n",
        "            #segpath = check_nifti_filepath(kwargs['atlasdir'], ('atlas%02d' % (i + 1)) + '_' + 'lesion')\n",
        "            segpath = check_nifti_filepath(kwargs['atlasdir'], ('atlas%s' % (i)) + '_' + 'mask') #---efm---\n",
        "            mask = np.transpose(pad_image(nib.load(segpath).get_fdata(), padsize),\n",
        "                                axes=transpose_code).astype(np.float32)\n",
        "            vol4d = np.zeros(mask.shape + (nummodal,), dtype=np.float32)\n",
        "            \n",
        "            # loading volume & normalize (T1 & FL)\n",
        "            for j in range(nummodal):\n",
        "                filepath = check_nifti_filepath(kwargs['atlasdir'],\n",
        "                                                ('atlas%s' % (i)) + '_' + kwargs['modalities'][j].upper())\n",
        "                #                                ('atlas%02d' % (i + 1)) + '_' + kwargs['modalities'][j].lower())\n",
        "                print('Reading %s' % filepath)\n",
        "                vol4d[:, :, :, j] = np.transpose(pad_image(normalize_image(nib.load(filepath).get_fdata(),kwargs['modalities'][j]), padsize),\n",
        "                                                 axes=transpose_code).astype(np.float32)\n",
        "            print('Atlas %s size = %d x %d x %d x %d ' % ((i,) + vol4d.shape))\n",
        "\n",
        "            # Get 4D patches size\n",
        "            image_patches_a, mask_patches_a = get_patches(vol4d, mask, kwargs)\n",
        "            num_patches = image_patches_a.shape[0]\n",
        "            print('Atlas %s : indices [%d,%d)' % (i, patch_count, patch_count + num_patches))\n",
        "            image_patches[patch_count:patch_count + num_patches, :, :, :] = image_patches_a\n",
        "            mask_patches[patch_count:patch_count + num_patches, :, :, :] = mask_patches_a\n",
        "            patch_count += num_patches\n",
        "            print('-' * 100)\n",
        "\n",
        "        image_patches = image_patches[0:patch_count, :, :, :]\n",
        "        mask_patches = mask_patches[0:patch_count, :, :, :]\n",
        "\n",
        "        \"\"\"\n",
        "        If the number of patches within training & validation sets are not multiple of number of GPU,\n",
        "        there could be some arbitrary CUDNN error. Although ideally this should be taken care within\n",
        "        multi_gpu_model function, it is not; discarding last couple of samples is easiest.\n",
        "        This will not work if the number of samples is low, e.g. marmosets or CT. Use single gpu in those cases.\n",
        "        \"\"\"\n",
        "        numgpu = kwargs['numgpu']\n",
        "        if numgpu>1:\n",
        "            L = image_patches.shape[0]\n",
        "            L = np.floor(np.floor(L * 0.2) / numgpu) * numgpu * 5\n",
        "            L = np.asarray(L, dtype=int)\n",
        "            image_patches = image_patches[0:L, :, :, :]\n",
        "            mask_patches = mask_patches[0:L, :, :, :]\n",
        "\n",
        "        print('Total number of patches collected = ' + str(patch_count))\n",
        "        print('Sizes of the input matrices are ' + str(image_patches.shape) + ' and ' + str(mask_patches.shape))\n",
        "\n",
        "        # if len(kwargs['patchsize']) == 2:\n",
        "        #     model = get_model_2d(kwargs['base_filters'], nummodal, kwargs['numgpu'], kwargs['loss'])\n",
        "        # else:\n",
        "        #     model = get_model_3d(kwargs['base_filters'], nummodal, kwargs['numgpu'], kwargs['loss'])\n",
        "        print(nummodal)\n",
        "        model = get_model_3d(kwargs['base_filters'], nummodal, kwargs['numgpu'], kwargs['loss'])\n",
        "        print('model 3d created')\n",
        "\n",
        "        if kwargs['loss'] == 'bce':\n",
        "            callbacks = [ModelCheckpoint(tempoutname, monitor='val_accuracy', verbose=1, save_best_only=True,\n",
        "                                     period=kwargs['period'], mode='max')] if kwargs['period'] > 0 else None\n",
        "            dlr = ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.5, patience=2,\n",
        "                                mode='max', verbose=1, cooldown=2, min_lr=1e-8)\n",
        "            earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.0002, patience=5,\n",
        "                                  verbose=1, mode='max')\n",
        "        else:\n",
        "\n",
        "            callbacks = [ModelCheckpoint(tempoutname, monitor='val_loss', verbose=1, save_best_only=True,\n",
        "                                         period=kwargs['period'], mode='min')] if kwargs['period'] > 0 else None\n",
        "            dlr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2,\n",
        "                                    mode='min', verbose=1, cooldown=2, min_lr=1e-8)\n",
        "            earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0002, patience=10,\n",
        "                                      verbose=1, mode='min')\n",
        "        callbacks.append(dlr)\n",
        "        callbacks.append(earlystop)\n",
        "\n",
        "        if kwargs['initmodel'] != 'None' and os.path.exists(kwargs['initmodel']):\n",
        "            dict = {\"tf\": tf,\n",
        "                    }\n",
        "            oldmodel = load_model(kwargs['initmodel'],custom_objects=dict)\n",
        "            model.set_weights(oldmodel.get_weights())\n",
        "            print(\"Initializing from existing model %s\" % (kwargs['initmodel']))\n",
        "        # efm\n",
        "        # image_patches = np.expand_dims(image_patches, axis = 1)\n",
        "        model.fit(image_patches, mask_patches, batch_size=kwargs['batchsize'], epochs=kwargs['epoch'], verbose=1,\n",
        "                  validation_split=0.2, callbacks=callbacks, shuffle=True)\n",
        "        # shuffle = True should be used, but sometimes that results in instability. More experimentation needed\n",
        "        \n",
        "        print('Final model is written at ' + orient_outname)\n",
        "        # if len(kwargs['gpu_ids']) > 1:\n",
        "        #     \"\"\"\n",
        "        #     Training with multi-gpu model has a bug where the trained model can not be loaded to a single gpu for\n",
        "        #     testing, giving some zero batch size errors, such as\n",
        "        #         1) could not convert BatchDescriptor {count: 0 ...\n",
        "        #         2) cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\n",
        "        #     Therefore the following fix works when models trained with multi-gpu are used for single gpu.\n",
        "        #     It is obtained from the following link,\n",
        "        #        https://stackoverflow.com/questions/47210811/can-not-save-model-using-model-save-following-multi-\n",
        "        #        gpu-model-in-keras/48066771#48066771\n",
        "        #     ** This is fixed in Keras 2.1.5, although it is better for backwards compatibility **\n",
        "        #     ** Don't use default multi_gpu_model from Keras 2.1.5, sometimes the resulting model file can't be read. \n",
        "        #     Not sure why.**\n",
        "        #     \"\"\"\n",
        "\n",
        "        #     with tf.device(\"/cpu:0\"):\n",
        "        #         single_model = get_model_2d(kwargs['base_filters'], nummodal, 1, kwargs['loss']) if len(kwargs['patchsize']) == 2 \\\n",
        "        #             else get_model_3d(kwargs['base_filters'], nummodal, 1, kwargs['loss'])\n",
        "        #     if kwargs['loss'] == 'bce':\n",
        "        #         single_model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        #     else:\n",
        "        #         single_model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
        "        #     single_model.set_weights(model.get_weights())\n",
        "        #     single_model.save(filepath=orient_outname)\n",
        "        # else:\n",
        "        model.save(filepath=orient_outname,  overwrite=True, include_optimizer=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbcjTRH9s7K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "natlas = 6\n",
        "outdir = \"/content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/Checkpoints\"\n",
        "modalities = ['T1', 'FL']\n",
        "psize = [100,100,100]\n",
        "atlasdir = \"/content/gdrive/My Drive/UOC/TFM-Elisa-2020/IMAGES/AT\"\n",
        "batchsize = 64\n",
        "epoch = 50\n",
        "save = 1\n",
        "axialonly = False \n",
        "basefilters = 8\n",
        "maxpatches = 50000\n",
        "gpuids = None\n",
        "gpu_ids = None\n",
        "numgpu = 1\n",
        "loss = 'bce'\n",
        "INITMODEL = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oItBedbQi_Tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if gpuids is not None:\n",
        "    gpu_ids = gpuids\n",
        "else:\n",
        "    gpu_ids = range(numgpu)\n",
        "\n",
        "numgpu = len(gpu_ids)\n",
        "batchsize = (batchsize // numgpu) * numgpu\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDicTSRAhVQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kwargs = { 'numatlas': natlas,\n",
        "        'outdir': outdir,\n",
        "        'modalities': [item.upper() for item in modalities],\n",
        "        'patchsize': tuple(psize),\n",
        "        'atlasdir': atlasdir,\n",
        "        'batchsize': batchsize,\n",
        "        'epoch': epoch,\n",
        "        'period': save,\n",
        "        'axial_only': axialonly,\n",
        "        'base_filters': basefilters,\n",
        "        'max_patches': maxpatches,\n",
        "        'gpu_ids': 1,\n",
        "        'numgpu': 1,\n",
        "        'loss': str(loss).lower(),\n",
        "        'initmodel': INITMODEL,\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88Vz9BHPhXtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nummodal = int(len(kwargs['modalities']))\n",
        "padsize = np.max(np.array(kwargs['patchsize']) + 1) / 2\n",
        "kwargs['batchsize'] = kwargs['batchsize'] * kwargs['numgpu']\n",
        "f = 0 # num_patches\n",
        "#--efm\n",
        "atlas = os.listdir(kwargs['atlasdir'])\n",
        "nums = []\n",
        "for im in atlas:\n",
        "    nums.append(re.findall('\\d+', im)[0])\n",
        "N = np.unique(nums)\n",
        "#---"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKQ8k3ZthbEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6fe76452-8e06-431a-f1e9-f400d92cd472"
      },
      "source": [
        "\n",
        "for i in N:\n",
        "    maskname = check_nifti_filepath(kwargs['atlasdir'], ('atlas%s' % (i)) + '_' + 'mask')\n",
        "    f += min(kwargs['max_patches'], nib.load(maskname).get_fdata().sum())\n",
        "print('Total number of lesion patches = ' + str(int(f)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of lesion patches = 20429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5JUbMZBhtmf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "1a276c0a-5775-4616-ff19-795cc68d731e"
      },
      "source": [
        "r = 1 # ratio between positive and negative patches\n",
        "mask_patches = tf.zeros((int((r+1)*f),) + kwargs['patchsize'] + (1,), dtype=tf.dtypes.float32)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b613be3377a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# ratio between positive and negative patches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmask_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'patchsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   1836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure it's a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mfill\u001b[0;34m(dims, value, name)\u001b[0m\n\u001b[1;32m   3541\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3542\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3543\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3544\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3545\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[40858,100,100,100,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill] name: zeros/"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGEZqna3hiHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "image_patches = np.zeros((int((r+1)*f),) + kwargs['patchsize'] + (nummodal,), dtype=np.float32)\n",
        "\n",
        "time_id = time.strftime('%d-%m-%Y_%H-%M-%S')\n",
        "print('Unique ID is %s ' % time_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMvSU3CrhKCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "r = 1 # ratio between positive and negative patches\n",
        "mask_patches = np.zeros((int((r+1)*f),) + kwargs['patchsize'] + (1,), dtype=np.float32)\n",
        "image_patches = np.zeros((int((r+1)*f),) + kwargs['patchsize'] + (nummodal,), dtype=np.float32)\n",
        "\n",
        "time_id = time.strftime('%d-%m-%Y_%H-%M-%S')\n",
        "print('Unique ID is %s ' % time_id)\n",
        "\n",
        "con = '+'.join([str(mod).upper() for mod in kwargs['modalities']])\n",
        "psize = 'x'.join([str(side) for side in kwargs['patchsize']])\n",
        "outname = 'FLEXCONN_Model_' + psize + '_Orient%d%d%d_' + con + '_' + time_id + '.h5'\n",
        "\n",
        "codes = [(0, 1, 2), (1, 2, 0), (2, 0, 1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX9RN08RhKF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj139BEBhKAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OLg6aoCKLGu",
        "colab_type": "code",
        "outputId": "5a3889d5-0b7f-41bf-fe16-42aa7afbdc42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Patch size must be odd\n",
        "for i in range(len(psize)):\n",
        "    psize[i] = (psize[i]//2)*2 + 1\n",
        "\n",
        "if INITMODEL is None:\n",
        "    INITMODEL = 'None'\n",
        "\n",
        "opt = { 'numatlas': natlas,\n",
        "        'outdir': outdir,\n",
        "        'modalities': [item.upper() for item in modalities],\n",
        "        'patchsize': tuple(psize),\n",
        "        'atlasdir': atlasdir,\n",
        "        'batchsize': batchsize,\n",
        "        'epoch': epoch,\n",
        "        'period': save,\n",
        "        'axial_only': axialonly,\n",
        "        'base_filters': basefilters,\n",
        "        'max_patches': maxpatches,\n",
        "        'gpu_ids': 1,\n",
        "        'numgpu': 1,\n",
        "        'loss': str(loss).lower(),\n",
        "        'initmodel': INITMODEL,\n",
        "        }\n",
        "\n",
        "main(opt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of lesion patches = 20429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSwyaHFkuq2y",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzAXEPpUuq6T",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IDFz37suq9e",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyFJiSZHurAt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYNwLnBntESl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vfFg3U0JtLBg",
        "colab": {}
      },
      "source": [
        "models = \"/content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/Models/*.h5\"\n",
        "inputdir = \"/content/gdrive/My Drive/UOC/TFM-Elisa-2020/IMAGES/Atlas_train\"\n",
        "imgs = [\"atlas12_T1.nii.gz\",\"atlas12_FL.nii.gz\",\"atlas13_T1.nii.gz\", \"atlas13_FL.nii.gz\",\"atlas16_T1.nii.gz\", \"atlas16_FL.nii.gz\"]\n",
        "modalities = ['T1', 'FL']\n",
        "psize = [100,100]\n",
        "outdir = \"/content/gdrive/My Drive/UOC/TFM-Elisa-2020/FLEXCONN_CODE/Results\"\n",
        "gpu = 1\n",
        "threshold = 0.5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrWLP2APEjez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9cWxlsss6Mb",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}